2章 時系列データの見つけ方と前処理
================
@nozma
2022-08-17

## 2.2 表データの集合から時系列データの集合を作成する

-   書籍のGitHubリポジトリからデータ読み込み。
-   以下の調整を行った。
    -   `user`、`emailsOpens`は整数型に変換。
    -   `week`は日付型に変換。

``` r
yearJoined <- read_csv(
  "https://raw.githubusercontent.com/PracticalTimeSeriesAnalysis/BookRepo/master/Ch02/data/year_joined.csv",
  col_types = "ici"
)
emails <- read_csv(
  "https://raw.githubusercontent.com/PracticalTimeSeriesAnalysis/BookRepo/master/Ch02/data/emails.csv",
  col_types = "nnT"
) %>% 
  mutate(
    emailsOpened = as.integer(emailsOpened),
    user = as.integer(user),
    week = as.Date(week)
  )
donations <- read_csv(
  "https://raw.githubusercontent.com/PracticalTimeSeriesAnalysis/BookRepo/master/Ch02/data/donations.csv",
  col_types = "nTn"
) %>% mutate(user = as.integer(user))
```

### 2.2.1 事例：収集した時系列データを組み立てる

#### p.25 `yearJoined`のレコードが会員毎に何件あるかを確認する

``` r
yearJoined %>% count(user) %>% distinct(n)
```

    ## # A tibble: 1 × 1
    ##       n
    ##   <int>
    ## 1     1

#### p.26 ユーザーがメールを開いていない週のレコードが存在するかどうかを確認する

まず、0件のデータがないことを確認。

``` r
emails %>% filter(emailsOpened < 1)
```

    ## # A tibble: 0 × 3
    ## # … with 3 variables: emailsOpened <int>, user <int>, week <date>
    ## # ℹ Use `colnames()` to see all variable names

次に特定のユーザーのデータを見て、欠損があることを確認。

-   書籍では生のデータを確認していたが、日付の差分をとり間隔が7日ではないデータが存在することを確認した。
-   日付の範囲から期待されるレコード数を求める方法は直感的ではなく間違いにつながる可能性(※)があるため略。
    -   ※意図しないレコードが挿入されていたり、レコードが重複しているような場合を発見できない可能性がある。

``` r
emails %>% 
  filter(user == 998) %>% 
  arrange(week) %>% 
  mutate(diff = week - lag(week)) %>% 
  count(diff)
```

    ## # A tibble: 3 × 2
    ##   diff        n
    ##   <drtn>  <int>
    ## 1  7 days    21
    ## 2 14 days     2
    ## 3 NA days     1

#### p.27 会員データの欠損週を埋める

書籍では日付とユーザーIDの直積を求め、それをインデックスに設定する形で欠損週のデータを生成している。

しかし、これには次のような課題がある。

-   すべてのユーザーがメールを開かなかった週が存在する場合、その週は欠損したままである。
-   ユーザーごとに先頭・末尾に不要なレコードが生ずるため、処理が手間である。
    -   書籍では、これを後の作業で取り除いている。

そこで、次のように行った。

-   ユーザーごとに`week`の最大・最小を求める。
-   `purrr::map2`で1週間ごとの日付を生成し、`tidyr::unnest`で展開。
-   元のデータを再度結合し、`tidyr::replace_na`でNAを0に置換。

``` r
emails %>% 
  group_by(user) %>% 
  # ユーザーごとに最大・最小の日付を取得
  summarise(
    start_date = min(week), 
    end_date = max(week)
  ) %>% 
  # 最大・最小の範囲から1週間ごとの日付を生成
  mutate(
    week = map2(start_date, end_date, ~seq(.x, .y, by = "1 week"))
  ) %>% 
  unnest(week) %>% 
  select(user, week) %>% 
  # emailsデータを結合してNAを0で置換
  left_join(emails, by = c("user", "week")) %>% 
  replace_na(list(emailsOpened = 0)) -> all_email
```

### 2.2.2 発掘した時系列の構築

#### p.30 寄付額データを1週間単位に変換

`lubridate::round_date`でタイムスタンプを1週間単位にしてから集約。

``` r
donations %>% 
  mutate(
    week = ceiling_date(timestamp, unit = "week", week_start = 1)
  ) %>% 
  group_by(user, week) %>% 
  summarise(
    amount = sum(amount),
    .groups = "drop"
  ) -> agg_donations
```

#### p.30-31 メール開封データと寄付額データを結合する

書籍のPythonコードはかなり修正しないと動かなかった。

p.31でターゲット変数のシフトも行っているので、それもついでに実施する。

``` r
all_email %>% 
  left_join(agg_donations, by = c("user", "week")) %>% 
  group_by(user) %>% 
  mutate(
    target = lag(amount)
  ) %>% 
  replace_na(list(amount = 0, target = 0)) -> merged_df
```

``` r
merged_df %>% 
  filter(user == 998)
```

    ## # A tibble: 26 × 5
    ## # Groups:   user [1]
    ##     user week                emailsOpened amount target
    ##    <int> <dttm>                     <int>  <dbl>  <dbl>
    ##  1   998 2017-12-04 00:00:00            1      0      0
    ##  2   998 2017-12-11 00:00:00            3      0      0
    ##  3   998 2017-12-18 00:00:00            3      0      0
    ##  4   998 2017-12-25 00:00:00            0      0      0
    ##  5   998 2018-01-01 00:00:00            3      0      0
    ##  6   998 2018-01-08 00:00:00            3     50      0
    ##  7   998 2018-01-15 00:00:00            2      0     50
    ##  8   998 2018-01-22 00:00:00            3      0      0
    ##  9   998 2018-01-29 00:00:00            2      0      0
    ## 10   998 2018-02-05 00:00:00            3      0      0
    ## # … with 16 more rows
    ## # ℹ Use `print(n = ...)` to see more rows

## 2.4 データのクリーニング

### 2.4.1 欠損値の処理

#### p.38 データの準備

もともとRのコードだが、本文のコードは下から2行目が誤っている。

-   誤: `high.unemp.idx <- sample(high.unemp.idx,)`
-   正: `high.unemp.idx <- sample(high.unemp.idx, num.to.select)`

その他の記述もやや冗長なので書き直した。

``` r
unemp <- read_csv(
  "https://raw.githubusercontent.com/PracticalTimeSeriesAnalysis/BookRepo/master/Ch02/data/UNRATE.csv",
  col_types = "Dn"
)

# 無作為に欠損させたデータセットを生成する
set.seed(778)
unemp %>% 
  slice_sample(prop = 0.9) -> rand_unemp

# 失業率が高い月に欠損している確率が高いデータセットを生成する
unemp %>% 
  anti_join(
    # 失業率8を超えるレコードの20%を抽出し、anti_joinで除外
    unemp %>% filter(UNRATE > 8) %>% slice_sample(prop = 0.2),
    by = c("DATE", "UNRATE")
  ) -> bias_unemp
```

#### p.40 欠損させたデータの日付とNAを補う

書籍ではrolling
joinを使用しているが、使わなくとも可能かつ現行バージョンのdplyrではrolling
joinができないので別の方法で埋める。

``` r
all_dates <- seq(min(unemp$DATE), max(unemp$DATE), by = "1 month")
tibble(DATE = all_dates) %>% 
  left_join(rand_unemp, by = "DATE") %>% 
  mutate(missing = is.na(UNRATE)) -> rand_unemp
tibble(DATE = all_dates) %>% 
  left_join(bias_unemp, by = "DATE") %>% 
  mutate(missing = is.na(UNRATE)) -> bias_unemp
```

#### p.41-42 前方埋め

前方埋めをした上でp.41の作図まで行う。

``` r
unemp %>% 
  mutate(group = "original") %>% 
  union_all(
    rand_unemp %>% 
      fill(UNRATE, .direction = "down") %>% # 前方埋め
      mutate(group = "random missing")
  ) %>% 
  ggplot(aes(x = DATE, y = UNRATE, color = group)) +
  geom_line() +
  geom_point(data = . %>% filter(missing)) +
  lims(x = c(ymd("1977/01/01"), ymd("1981/12/31")))
```

    ## Warning: Removed 1568 row(s) containing missing values (geom_path).

    ## Warning: Removed 80 rows containing missing values (geom_point).

![](ch02_files/figure-gfm/unnamed-chunk-11-1.png)<!-- -->

p.42の作図

``` r
unemp %>% 
  rename(original = UNRATE) %>% 
  left_join(
    rand_unemp %>% 
      fill(UNRATE, .direction = "down") %>% # 前方埋め
      rename(random_missing = UNRATE),
    by = "DATE"
  ) %>% 
  ggplot(aes(x = original, y = random_missing, color = missing)) +
  geom_point()
```

![](ch02_files/figure-gfm/unnamed-chunk-12-1.png)<!-- -->

#### p.44 移動平均

先読みをしない移動平均による補完。先読みをしない移動平均は`zoo::rollmean()`で`align = "right"`を指定するか、`zoo::rollmeanr()`を使う。

ただし、この方法では移動平均の範囲を上回って連続するNAが出現すると結果がNaNとなる。`imputeTS::na_ma()`ではそのような場合に移動平均の範囲を広げるアルゴリズムが採用されている。しかし、この関数は先読みなしの移動平均に対応していない。

``` r
rand_unemp %>% 
  mutate(
    rmean = rollmeanr(UNRATE, 3, fill = NA, na.rm = TRUE),
    UNRATE = if_else(missing, rmean, UNRATE)
  ) %>% 
  select(!rmean) -> smooth_1
smooth_1
```

    ## # A tibble: 844 × 3
    ##    DATE       UNRATE missing
    ##    <date>      <dbl> <lgl>  
    ##  1 1948-01-01   3.4  FALSE  
    ##  2 1948-02-01   3.8  FALSE  
    ##  3 1948-03-01   4    FALSE  
    ##  4 1948-04-01   3.9  TRUE   
    ##  5 1948-05-01   3.5  FALSE  
    ##  6 1948-06-01   3.6  FALSE  
    ##  7 1948-07-01   3.6  FALSE  
    ##  8 1948-08-01   3.9  FALSE  
    ##  9 1948-09-01   3.75 TRUE   
    ## 10 1948-10-01   3.7  FALSE  
    ## # … with 834 more rows
    ## # ℹ Use `print(n = ...)` to see more rows

``` r
# bias_unempは略
```

先読みをする移動平均。

``` r
rand_unemp %>% 
  mutate(
    rmean = rollmean(UNRATE, 3, fill = NA, na.rm = TRUE),
    UNRATE = if_else(missing, rmean, UNRATE)
  ) %>% 
  select(!rmean) -> smooth_2
smooth_2
```

    ## # A tibble: 844 × 3
    ##    DATE       UNRATE missing
    ##    <date>      <dbl> <lgl>  
    ##  1 1948-01-01   3.4  FALSE  
    ##  2 1948-02-01   3.8  FALSE  
    ##  3 1948-03-01   4    FALSE  
    ##  4 1948-04-01   3.75 TRUE   
    ##  5 1948-05-01   3.5  FALSE  
    ##  6 1948-06-01   3.6  FALSE  
    ##  7 1948-07-01   3.6  FALSE  
    ##  8 1948-08-01   3.9  FALSE  
    ##  9 1948-09-01   3.8  TRUE   
    ## 10 1948-10-01   3.7  FALSE  
    ## # … with 834 more rows
    ## # ℹ Use `print(n = ...)` to see more rows

p.45の作図。

``` r
unemp %>% mutate(group = "original") %>% 
  union_all(smooth_1 %>% mutate(group = "look ahead = FALSE")) %>% 
  union_all(smooth_2 %>% mutate(group = "look ahead = TRUE")) %>% 
  ggplot(aes(x = DATE, y = UNRATE, color = group)) +
  geom_point(data = . %>% filter(missing)) +
  lims(x = c(ymd("1952/01/01"), ymd("1956/12/31")), y = c(2.5, 6.5)) +
  geom_line()
```

    ## Warning: Removed 160 rows containing missing values (geom_point).

    ## Warning: Removed 2352 row(s) containing missing values (geom_path).

![](ch02_files/figure-gfm/unnamed-chunk-15-1.png)<!-- -->

#### p.46 補間

線形補間と多項式補間。`imputeTS::na_interpolation()`を使った。

``` r
unemp %>% mutate(group = "original") %>% 
  union_all(
    rand_unemp %>% 
      mutate(
        group = "liner",
        UNRATE = na_interpolation(UNRATE) # 線形補間
      )
  ) %>% 
  union_all(
    rand_unemp %>% 
      mutate(
        group = "spline",
        UNRATE = na_interpolation(UNRATE, option = "spline") # 多項式(スプライン)補完
      )
  ) %>% 
  ggplot(aes(x = DATE, y = UNRATE, color = group)) +
  geom_point(data = . %>% filter(missing)) +
  lims(x = c(ymd("1958/01/01"), ymd("1961/12/31")), y = c(3, 8)) +
  geom_line()
```

    ## Warning: Removed 166 rows containing missing values (geom_point).

    ## Warning: Removed 2388 row(s) containing missing values (geom_path).

![](ch02_files/figure-gfm/unnamed-chunk-16-1.png)<!-- -->

#### p.47 全体的な比較

各種の補完手法に対し、平均二乗誤差を計算する。

``` r
calc_mse <- function(target) {
  unemp %>% 
    mutate(
      # 前方埋め
      inpute_ff = target %>%
        fill(UNRATE, .direction = "down") %>% 
        pull(UNRATE), 
      # 移動平均(先読みなし)
      inpute_rm_nolookahead = target %>% 
        mutate(
          rmean = rollmeanr(UNRATE, 3, fill = NA, na.rm = TRUE),
          UNRATE = if_else(missing, rmean, UNRATE)
        ) %>% 
        pull(UNRATE),
      # 移動平均(先読みあり)
      inpute_rm_lookahead = target %>% 
        mutate(
          rmean = rollmean(UNRATE, 3, fill = NA, na.rm = TRUE),
          UNRATE = if_else(missing, rmean, UNRATE)
        ) %>% 
        pull(UNRATE),
      # 線形補間
      inpute_li = target %>% 
        mutate(
          group = "liner",
          UNRATE = na_interpolation(UNRATE)
        ) %>% 
        pull(UNRATE),
      # 多項式補間
      inpute_sp = target %>% 
        mutate(
          group = "spline",
          UNRATE = na_interpolation(UNRATE)
        ) %>% 
        pull(UNRATE),
    ) %>% 
    summarise_at(vars(starts_with("inpute")), ~mean((.x - UNRATE)^2, na.rm = TRUE))
}
```

``` r
calc_mse(rand_unemp)
```

    ## # A tibble: 1 × 5
    ##   inpute_ff inpute_rm_nolookahead inpute_rm_lookahead inpute_li inpute_sp
    ##       <dbl>                 <dbl>               <dbl>     <dbl>     <dbl>
    ## 1   0.00344               0.00466             0.00180   0.00140   0.00140

``` r
calc_mse(bias_unemp)
```

    ## # A tibble: 1 × 5
    ##   inpute_ff inpute_rm_nolookahead inpute_rm_lookahead inpute_li inpute_sp
    ##       <dbl>                 <dbl>               <dbl>     <dbl>     <dbl>
    ## 1   0.00143               0.00161            0.000504  0.000375  0.000375

### 2.4.2 アップサンプリングとダウンサンプリング
