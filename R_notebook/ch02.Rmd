---
title: "2章 時系列データの見つけ方と前処理"
author: '@nozma'
date: "2022-08-17"
output: github_document
---

```{r setup, include=TRUE, message=FALSE}
knitr::opts_chunk$set(warning = FALSE)

library(readr)
library(dplyr)
library(tidyr)
library(lubridate)
library(purrr)
library(ggplot2)
theme_set(theme_bw())
library(zoo)
library(imputeTS)
library(reticulate)
library(ggfortify)
```

## 2.2 表データの集合から時系列データの集合を作成する

- 書籍のGitHubリポジトリからデータ読み込み。
- 以下の調整を行った。
  - `user`、`emailsOpens`は整数型に変換。
  - `week`は日付型に変換。

```{r}
yearJoined <- read_csv(
  "https://raw.githubusercontent.com/PracticalTimeSeriesAnalysis/BookRepo/master/Ch02/data/year_joined.csv",
  col_types = "ici"
)
emails <- read_csv(
  "https://raw.githubusercontent.com/PracticalTimeSeriesAnalysis/BookRepo/master/Ch02/data/emails.csv",
  col_types = "nnT"
) %>% 
  mutate(
    emailsOpened = as.integer(emailsOpened),
    user = as.integer(user),
    week = as.Date(week)
  )
donations <- read_csv(
  "https://raw.githubusercontent.com/PracticalTimeSeriesAnalysis/BookRepo/master/Ch02/data/donations.csv",
  col_types = "nTn"
) %>% mutate(user = as.integer(user))
```

### 2.2.1 事例：収集した時系列データを組み立てる

#### p.25 `yearJoined`のレコードが会員毎に何件あるかを確認する

```{r}
yearJoined %>% count(user) %>% distinct(n)
```

#### p.26 ユーザーがメールを開いていない週のレコードが存在するかどうかを確認する

まず、0件のデータがないことを確認。

```{r}
emails %>% filter(emailsOpened < 1)
```

次に特定のユーザーのデータを見て、欠損があることを確認。

- 書籍では生のデータを確認していたが、日付の差分をとり間隔が7日ではないデータが存在することを確認した。
- 日付の範囲から期待されるレコード数を求める方法は直感的ではなく間違いにつながる可能性(※)があるため略。
  - ※意図しないレコードが挿入されていたり、レコードが重複しているような場合を発見できない可能性がある。

```{r}
emails %>% 
  filter(user == 998) %>% 
  arrange(week) %>% 
  mutate(diff = week - lag(week)) %>% 
  count(diff)
```

#### p.27 会員データの欠損週を埋める

書籍では日付とユーザーIDの直積を求め、それをインデックスに設定する形で欠損週のデータを生成している。

しかし、これには次のような課題がある。

- すべてのユーザーがメールを開かなかった週が存在する場合、その週は欠損したままである。
- ユーザーごとに先頭・末尾に不要なレコードが生ずるため、処理が手間である。
  - 書籍では、これを後の作業で取り除いている。

そこで、次のように行った。

- ユーザーごとに`week`の最大・最小を求める。
- `purrr::map2`で1週間ごとの日付を生成し、`tidyr::unnest`で展開。
- 元のデータを再度結合し、`tidyr::replace_na`でNAを0に置換。

```{r}
emails %>% 
  group_by(user) %>% 
  # ユーザーごとに最大・最小の日付を取得
  summarise(
    start_date = min(week), 
    end_date = max(week)
  ) %>% 
  # 最大・最小の範囲から1週間ごとの日付を生成
  mutate(
    week = map2(start_date, end_date, ~seq(.x, .y, by = "1 week"))
  ) %>% 
  unnest(week) %>% 
  select(user, week) %>% 
  # emailsデータを結合してNAを0で置換
  left_join(emails, by = c("user", "week")) %>% 
  replace_na(list(emailsOpened = 0)) -> all_email
```

### 2.2.2 発掘した時系列の構築

#### p.30 寄付額データを1週間単位に変換

`lubridate::round_date`でタイムスタンプを1週間単位にしてから集約。

```{r}
donations %>% 
  mutate(
    week = ceiling_date(timestamp, unit = "week", week_start = 1)
  ) %>% 
  group_by(user, week) %>% 
  summarise(
    amount = sum(amount),
    .groups = "drop"
  ) -> agg_donations
```

#### p.30-31 メール開封データと寄付額データを結合する

書籍のPythonコードはかなり修正しないと動かなかった。

p.31でターゲット変数のシフトも行っているので、それもついでに実施する。

```{r}
all_email %>% 
  left_join(agg_donations, by = c("user", "week")) %>% 
  group_by(user) %>% 
  mutate(
    target = lag(amount)
  ) %>% 
  replace_na(list(amount = 0, target = 0)) -> merged_df
```

```{r}
merged_df %>% 
  filter(user == 998)
```

## 2.4 データのクリーニング

### 2.4.1 欠損値の処理

#### p.38 データの準備

もともとRのコードだが、本文のコードは下から2行目が誤っている。

- 誤: `high.unemp.idx <- sample(high.unemp.idx,)`
- 正: `high.unemp.idx <- sample(high.unemp.idx, num.to.select)`

その他の記述もやや冗長なので書き直した。

```{r}
unemp <- read_csv(
  "https://raw.githubusercontent.com/PracticalTimeSeriesAnalysis/BookRepo/master/Ch02/data/UNRATE.csv",
  col_types = "Dn"
)

# 無作為に欠損させたデータセットを生成する
set.seed(778)
unemp %>% 
  slice_sample(prop = 0.9) -> rand_unemp

# 失業率が高い月に欠損している確率が高いデータセットを生成する
unemp %>% 
  anti_join(
    # 失業率8を超えるレコードの20%を抽出し、anti_joinで除外
    unemp %>% filter(UNRATE > 8) %>% slice_sample(prop = 0.2),
    by = c("DATE", "UNRATE")
  ) -> bias_unemp
```


#### p.40 欠損させたデータの日付とNAを補う

書籍ではrolling joinを使用しているが、使わなくとも可能かつ現行バージョンのdplyrではrolling joinができないので別の方法で埋める。

```{r}
all_dates <- seq(min(unemp$DATE), max(unemp$DATE), by = "1 month")
tibble(DATE = all_dates) %>% 
  left_join(rand_unemp, by = "DATE") %>% 
  mutate(missing = is.na(UNRATE)) -> rand_unemp
tibble(DATE = all_dates) %>% 
  left_join(bias_unemp, by = "DATE") %>% 
  mutate(missing = is.na(UNRATE)) -> bias_unemp
```


#### p.41-42 前方埋め

前方埋めをした上でp.41の作図まで行う。

```{r}
unemp %>% 
  mutate(group = "original") %>% 
  union_all(
    rand_unemp %>% 
      fill(UNRATE, .direction = "down") %>% # 前方埋め
      mutate(group = "random missing")
  ) %>% 
  ggplot(aes(x = DATE, y = UNRATE, color = group)) +
  geom_line() +
  geom_point(data = . %>% filter(missing)) +
  lims(x = c(ymd("1977/01/01"), ymd("1981/12/31")))
```

p.42の作図

```{r}
unemp %>% 
  rename(original = UNRATE) %>% 
  left_join(
    rand_unemp %>% 
      fill(UNRATE, .direction = "down") %>% # 前方埋め
      rename(random_missing = UNRATE),
    by = "DATE"
  ) %>% 
  ggplot(aes(x = original, y = random_missing, color = missing)) +
  geom_point()
```


#### p.44 移動平均

先読みをしない移動平均による補完。先読みをしない移動平均は`zoo::rollmean()`で`align = "right"`を指定するか、`zoo::rollmeanr()`を使う。

ただし、この方法では移動平均の範囲を上回って連続するNAが出現すると結果がNaNとなる。`imputeTS::na_ma()`ではそのような場合に移動平均の範囲を広げるアルゴリズムが採用されている。しかし、この関数は先読みなしの移動平均に対応していない。

```{r}
rand_unemp %>% 
  mutate(
    rmean = rollmeanr(UNRATE, 3, fill = NA, na.rm = TRUE),
    UNRATE = if_else(missing, rmean, UNRATE)
  ) %>% 
  select(!rmean) -> smooth_1
smooth_1
# bias_unempは略
```

先読みをする移動平均。

```{r}
rand_unemp %>% 
  mutate(
    rmean = rollmean(UNRATE, 3, fill = NA, na.rm = TRUE),
    UNRATE = if_else(missing, rmean, UNRATE)
  ) %>% 
  select(!rmean) -> smooth_2
smooth_2
```

p.45の作図。

```{r}
unemp %>% mutate(group = "original") %>% 
  union_all(smooth_1 %>% mutate(group = "look ahead = FALSE")) %>% 
  union_all(smooth_2 %>% mutate(group = "look ahead = TRUE")) %>% 
  ggplot(aes(x = DATE, y = UNRATE, color = group)) +
  geom_point(data = . %>% filter(missing)) +
  lims(x = c(ymd("1952/01/01"), ymd("1956/12/31")), y = c(2.5, 6.5)) +
  geom_line()
```

#### p.46 補間

線形補間と多項式補間。`imputeTS::na_interpolation()`を使った。

```{r}
unemp %>% mutate(group = "original") %>% 
  union_all(
    rand_unemp %>% 
      mutate(
        group = "liner",
        UNRATE = na_interpolation(UNRATE) # 線形補間
      )
  ) %>% 
  union_all(
    rand_unemp %>% 
      mutate(
        group = "spline",
        UNRATE = na_interpolation(UNRATE, option = "spline") # 多項式(スプライン)補完
      )
  ) %>% 
  ggplot(aes(x = DATE, y = UNRATE, color = group)) +
  geom_point(data = . %>% filter(missing)) +
  lims(x = c(ymd("1958/01/01"), ymd("1961/12/31")), y = c(3, 8)) +
  geom_line()
```

#### p.47 全体的な比較

各種の補完手法に対し、平均二乗誤差を計算する。

```{r}
calc_mse <- function(target) {
  unemp %>% 
    mutate(
      # 前方埋め
      inpute_ff = target %>%
        fill(UNRATE, .direction = "down") %>% 
        pull(UNRATE), 
      # 移動平均(先読みなし)
      inpute_rm_nolookahead = target %>% 
        mutate(
          rmean = rollmeanr(UNRATE, 3, fill = NA, na.rm = TRUE),
          UNRATE = if_else(missing, rmean, UNRATE)
        ) %>% 
        pull(UNRATE),
      # 移動平均(先読みあり)
      inpute_rm_lookahead = target %>% 
        mutate(
          rmean = rollmean(UNRATE, 3, fill = NA, na.rm = TRUE),
          UNRATE = if_else(missing, rmean, UNRATE)
        ) %>% 
        pull(UNRATE),
      # 線形補間
      inpute_li = target %>% 
        mutate(
          group = "liner",
          UNRATE = na_interpolation(UNRATE)
        ) %>% 
        pull(UNRATE),
      # 多項式補間
      inpute_sp = target %>% 
        mutate(
          group = "spline",
          UNRATE = na_interpolation(UNRATE)
        ) %>% 
        pull(UNRATE),
    ) %>% 
    summarise_at(vars(starts_with("inpute")), ~mean((.x - UNRATE)^2, na.rm = TRUE))
}
```

```{r}
calc_mse(rand_unemp)
```

```{r}
calc_mse(bias_unemp)
```

### 2.4.2 アップサンプリングとダウンサンプリング

#### p.49 `unemp`から1月のデータのみ抜き出す。

```{r}
unemp %>% 
  filter(month(DATE) == 1)
```

#### p.49 年単位の平均をとる

```{r}
unemp %>% group_by(year(DATE)) %>% summarise(mean(UNRATE))
```

#### p.50 不規則な時系列をアップサンプリングする

```{r}
# p.40と同じ内容のため略
```

#### p.51 月初の値で月内の値を補う

月初から月末までの日付を生成して`unnest`。

```{r}
unemp %>% 
  mutate(
    DATE = map(DATE, ~seq(.x, .x + months(1) - days(1), by = "day"))
  ) %>% 
  unnest(DATE)
```

### 2.4.3 データの平滑化

#### p.54 指数平滑法

例示されている航空旅客データはRだと標準パッケージの`datasets`に含まれている`AirPassengers`が該当する。

```{r}
AirPassengers
```

pandasのpandas.DataFrame.ewmの`adjust=True`と同等の機能を持った関数を定義

```{r}
my_ewm <- function(x, a = NULL, adjust = TRUE) {
  nx <- length(x)
  if(is.null(a)) a = 2 / (nx + 1)
  y <- numeric(nx)
  y[1] <- x[1]
  if(adjust){
    a_adj <- 1
    w_sum <- x[1]
  }
  for (k in 2:nx) {
    if(adjust) {
      a_adj <- a_adj + (1 - a)^(k - 1)
      w_sum <- (x[k] + (1 - a) * w_sum)
      y[k] <-  w_sum / a_adj
    } else {
      y[k] <- a * x[k] + (1 - a) * y[k - 1]
    }
  }
  return(y)
}
```

pandasのewmをreticulateを使って呼び出し、自作の平滑化関数を比較し、結果が一致することを確認。

```{r}
pd <- import("pandas")

data.frame(
  Passengers = AirPassengers,
  my_ewm_5 = my_ewm(AirPassengers, 0.5),
  pd_ewm_5 = pd$DataFrame$ewm(
    data.frame(AirPassengers), 
    alpha = 0.5)$mean() %>% pull,
  my_ewm_9 = my_ewm(AirPassengers, 0.9),
  pd_ewm_9 = pd$DataFrame$ewm(
    data.frame(AirPassengers), 
    alpha = 0.9)$mean() %>% pull
  ) %>% head()
```

## 2.5 季節性データ

#### p.57のグラフ

```{r}
AirPassengers %>% plot(type = "p")
```


```{r}
AirPassengers %>% plot()
```

#### p.58のグラフ

```{r}
plot(stl(AirPassengers, "periodic"))
```

`ggfortify`を使う場合。

```{r}
autoplot(stl(AirPassengers, "periodic"))
```

## 2.6 タイムゾーン

#### p.60 

参考: [A Tour of Timezones (& Troubles) in R | R-bloggers](https://www.r-bloggers.com/2018/07/a-tour-of-timezones-troubles-in-r/)

`as.POSIXct()`はタイムゾーンを指定しないと、ローカルのタイムゾーンが自動で適用される。実行環境に依存するので割と危ない。

```{r}
x <- "2022-01-01 12:00:00"
as.POSIXct(x)
Sys.timezone()
as.POSIXct(x, tz = "America/Chicago")
```

`as.POSIXct()`でPOSIXctオブジェクトを作成した場合、`tz = `でタイムゾーンを指定しても時計の時刻は同じ。

```{r}
as.POSIXct(x, tz = "America/Chicago") - as.POSIXct(x)
```

`tz =`が不正なものである場合はGMT(UTC)で返る。以前は警告が出なかったらしいが今は出る。

```{r warning=TRUE}
as.POSIXct(x, tz = "America/Ypsilanti")
```

有効な名前は`OlsonName()`で一覧を得られるので、有効かどうかの判定はできる。

```{r}
c("America/Chicago", "America/Detroit", "America/Ypsilanti") %in% OlsonNames()
```

`lubridate`を使う場合。

`lubridate::as_datetime()`はデフォルトでUTCを返すので、実行環境によりタイムゾーンが意図せず変わる心配をしなくて良い。

```{r}
as_datetime(x)
```

生成されるオブジェクトはPOSIXctであるため、他のパッケージの関数に渡すような場合でも問題はない。

```{r}
as_datetime(x) %>% class()
```

タイムゾーンの指定も可。

```{r}
as_datetime(x, tz = "Asia/Tokyo")
```

正しくないタイムゾーンの名前を指定した場合、警告ではなくエラーが出るのでこの点も安心。

```{r error=TRUE}
as_datetime(x, tz = "Asia/Tokyu")
```

```{r}
# サンプルデータ生成
tbl <- tibble(datetime = ymd_hm("2018-03-10 23:30") + hours(c(1, 2, 4, 5, 6)))
```

```{r}
tz(tbl$datetime)
```

`lubridate::force_tz()`は表示されている時刻は変更せずにタイムゾーンを変更する(つまりclock timeが維持されて、UTCとしての時刻は変わる)。

```{r}
tbl %>% 
  mutate(datetime = force_tz(datetime, "America/New_York")) -> tbl
tbl
```

```{r}
tz(tbl$datetime)
```

```{r}
diff(tbl$datetime)
```

`lubridate::with_tz()`はUTCとしての時刻が維持されて、表示上の時刻であるclock timeを変更してタイムゾーンを変更する。

```{r}
tbl %>% 
  mutate(datetime = with_tz(datetime, "Asia/Tokyo")) -> tbl
tbl
```
```{r}
tz(tbl$datetime)
```

